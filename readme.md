# Симуляция работы пожарного дрона с использованием алгоритма Reinforcement Learning
# Описание проекта

## Среда (Environment)
Среда представляет собой прямоугольное поле размером **10×10** клеток, что составляет общее количество клеток:
\[N_{cells} = 10 \times 10 = 100\]

На этом поле могут находиться три типа клеток:
- **Клетки очагов огня** \(N_{fire}\) (красные) – источники пожара, которые агент должен ликвидировать.
- **Клетки препятствий** \(N_{obstacles}\) (черные) – недоступные для перемещения области.
- **Пустые клетки** (белые) – доступны для перемещения и взаимодействия.

**Ограничение:** суммарное количество красных и черных клеток не должно превышать 50% от общего числа клеток:
\[N_{fire} + N_{obstacles} \leq 0.5 \times N_{cells} = 50\]

**Агент** — это единственный оперативный дрон, который может перемещаться по полю и тушить очаги огня.

---

## Состояние (State)
В состояние входит:
- **Положение агента**: Координаты \((x_{agent}, y_{agent})\) на поле 10×10.
- **Конфигурация поля**: Матрица **10×10** \(grid\), где каждая клетка имеет метку:
  - `0` – пустая клетка (белая).
  - `1` – клетка с очагом огня (красная).
  - `2` – клетка-препятствие (черная).
  - `3` – клетка базы (стартовая клетка – в левом нижнем углу, зеленая).
- **Дополнительные параметры**:
  - Расстояния до всех видимых очагов огня.
  - Заряд батареи (от 0 до 100).
  - Доступные средства для тушения (`0` – нет, `1` – имеются (капсула или вода)).

**Формула состояния:**
\[S = [grid, (x_{agent}, y_{agent}), battery\_level, extinguisher\_count, distances\_to\_fires]\]

---

## Наблюдение (Observation)
Наблюдение — это частичная или полная информация из состояния, которую агент может "видеть" или воспринимать.

**В наблюдение входит:**
- **Видимость поля**: Агент может "видеть" только часть поля (радиус в 1 клетку вокруг себя).
- **Информация о текущей клетке**: Агент точно знает, находится ли он на клетке с очагом (`1`), препятствием (`2`), базой (`3`) или пустой (`0`).
- **Расстояния до очагов**: Если агент "видит" очаги в пределах своей зоны видимости, он получает информацию о их расположении.

Формула наблюдения:
\[O = [visible\_grid, (x_{agent}, y_{agent}), visible\_distances\_to\_fires]\]

---

## Действия (Action)
Агент может выполнять следующие действия:
- **Перемещение** (`move_up`, `move_down`, `move_left`, `move_right`), если:
  - Целевая клетка свободна \(grid[x_{target}, y_{target}] = 0\) или база (`3`).
  - Уровень заряда батареи достаточен \(battery\_level \geq \Delta battery\), где \(\Delta battery = 5\).
- **Тушение** (`extinguish`): Если агент находится на клетке с очагом (`1`) и у него есть средство для тушения (`extinguisher_count > 0`).
- **Перезарядка и пополнение ресурсов** (`recharge`, `refill`): Если агент находится на базе (`3`).

Пространство действий:
\[A = [move\_up, move\_down, move\_left, move\_right, extinguish, recharge, refill]\]

---

## Функция вознаграждения (Reward Function)
Функция вознаграждения стимулирует агента эффективно ликвидировать очаги огня, минимизируя время и ресурсы.

**Положительные вознаграждения:**
- \(R_{extinguish} = +10\) – успешное тушение очага огня.
- \(R_{detect\_fire} = +2\) – обнаружение очага в пределах радиуса 1 клетки.
- \(R_{move\_toward\_fire} = +1\) – движение в сторону очага огня.

**Отрицательные вознаграждения:**
- \(R_{move\_away\_from\_fire} = -2\) – движение в противоположную сторону от очага.
- \(R_{idle\_move} = -0.1\) – перемещение на пустую клетку без взаимодействия.
- \(R_{invalid\_move} = -3\) – попытка перемещения на препятствие или за границы поля.
- \(R_{wrong\_action} = -5\) – неудачная попытка тушения.
- \(R_{no\_extinguisher} = -5\) – попытка тушения без средств.
- \(R_{low\_battery} = -10\) – попытка перемещения с разряженной батареей.

\[R = \{+10, +2, +1, -2, -0.1, -3, -5, -5, -10\}\]

---

## Вывод
Проект моделирует среду для автономного агента, выполняющего задачи тушения пожаров. Среда включает ограниченную видимость, систему вознаграждений и ограничения ресурсов, что делает задачу интересной для алгоритмов обучения с подкреплением (RL).

